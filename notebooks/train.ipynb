{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJNLbR5sByvT"
      },
      "outputs": [],
      "source": [
        "# Ket noi google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "_xl9YMTauQPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lB9Ei_ZVJsv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from roboflow import Roboflow\n",
        "from ultralytics import YOLO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBZ8BfixuC9r"
      },
      "outputs": [],
      "source": [
        "ROOT = os.getcwd()\n",
        "ROOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgI-Y3nYXjhk"
      },
      "outputs": [],
      "source": [
        "DESTINATION_PATH = '/content/drive/MyDrive/Computer_Vision/Supermarket-people-1'\n",
        "os.listdir(DESTINATION_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C1LH909Xtgn"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "# Kiem tra file yaml\n",
        "with open (f\"{DESTINATION_PATH}/data.yaml\", 'r') as file:\n",
        "    # num_classes = str(yaml.safe_load(stream)['nc'])\n",
        "    config = yaml.safe_load(file)\n",
        "    print(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGIuclaLX_6u"
      },
      "outputs": [],
      "source": [
        "print(f\"So luong du lieu tap train: {len(os.listdir(f'{DESTINATION_PATH}/train/images'))}\")\n",
        "print(f\"So luong du lieu tap val: {len(os.listdir(f'{DESTINATION_PATH}/valid/images'))}\")\n",
        "print(f\"So luong du lieu tap val: {len(os.listdir(f'{DESTINATION_PATH}/test/images'))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlvZNHnPFOOd",
        "outputId": "36720b57-50c3-4ffa-d3d1-9cb1a71bd18f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.4MB 175.7MB/s 0.0s\n",
            "\n",
            "B·∫Øt ƒë·∫ßu qu√° tr√¨nh hu·∫•n luy·ªán YOLOv8...\n",
            "Ultralytics 8.3.205 üöÄ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/Computer_Vision/Supermarket-people-1/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=demo_run2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/Computer_Vision/Supermarket-people-1_trainer, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/Computer_Vision/Supermarket-people-1_trainer/demo_run2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 37.7MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=10\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    432622  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
            "YOLO11n summary: 181 layers, 2,591,790 parameters, 2,591,774 gradients, 6.5 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.6¬±0.2 ms, read: 0.0¬±0.0 MB/s, size: 13.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Computer_Vision/Supermarket-people-1/train/labels.cache... 906 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 906/906 1.4Mit/s 0.0s\n",
            "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 119, len(boxes) = 32128. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 1.0¬±0.8 ms, read: 0.0¬±0.0 MB/s, size: 15.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Computer_Vision/Supermarket-people-1/valid/labels.cache... 52 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 52/52 31.3Kit/s 0.0s\n",
            "Plotting labels to /content/drive/MyDrive/Computer_Vision/Supermarket-people-1_trainer/demo_run2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/Computer_Vision/Supermarket-people-1_trainer/demo_run2\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      1/100      3.59G      2.059      3.661      1.286        495        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 57/57 0.9it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 50% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 1/2 0.1it/s 5.1s<17.1s"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "\n",
        "# Kh·ªüi t·∫°o m√¥ h√¨nh YOLOv8\n",
        "# S·ª≠ d·ª•ng 'yolov8n.pt' (nano) ƒë·ªÉ b·∫Øt ƒë·∫ßu nhanh, ho·∫∑c 'yolov8s.pt' (small) cho hi·ªáu su·∫•t t·ªët h∆°n\n",
        "model = YOLO('yolo11n.pt')\n",
        "\n",
        "print(\"\\nB·∫Øt ƒë·∫ßu qu√° tr√¨nh hu·∫•n luy·ªán YOLOv8...\")\n",
        "\n",
        "# L·ªánh hu·∫•n luy·ªán\n",
        "# --data: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file data.yaml (ch·ª©a th√¥ng tin v·ªÅ ƒë∆∞·ªùng d·∫´n t·∫≠p train/val v√† s·ªë l∆∞·ª£ng class)\n",
        "# --epochs: S·ªë l∆∞·ª£ng epoch hu·∫•n luy·ªán\n",
        "# --imgsz: K√≠ch th∆∞·ªõc ·∫£nh ƒë·∫ßu v√†o (v√≠ d·ª• 640x640)\n",
        "# --batch: K√≠ch th∆∞·ªõc batch size (t√πy thu·ªôc v√†o GPU, 16 l√† m·ª©c an to√†n)\n",
        "# --project: Th∆∞ m·ª•c g·ªëc ƒë·ªÉ l∆∞u k·∫øt qu·∫£\n",
        "# --name: T√™n th∆∞ m·ª•c con c·ª• th·ªÉ cho l·∫ßn ch·∫°y n√†y\n",
        "\n",
        "DRIVE_DESTINATION_FOLDER = 'Computer_Vision'\n",
        "SOURCE_FOLDER_NAME = 'Supermarket-people-1'\n",
        "\n",
        "RESULTS_PATH = f\"/content/drive/MyDrive/{DRIVE_DESTINATION_FOLDER}/{SOURCE_FOLDER_NAME}_trainer\"\n",
        "drive_save_dir_trainer = os.path.dirname(RESULTS_PATH)\n",
        "os.makedirs(drive_save_dir_trainer, exist_ok=True)\n",
        "results = model.train(\n",
        "    data=f\"{DESTINATION_PATH}/data.yaml\",  # File c·∫•u h√¨nh d·ªØ li·ªáu t·ª´ Roboflow\n",
        "    epochs=100,            # S·ªë l∆∞·ª£ng epoch (th·ª≠ nghi·ªám v·ªõi 50-100)\n",
        "    imgsz=640,            # K√≠ch th∆∞·ªõc h√¨nh ·∫£nh\n",
        "    batch=16,             # K√≠ch th∆∞·ªõc batch\n",
        "    project=RESULTS_PATH, # L∆∞u k·∫øt qu·∫£ v√†o Google Drive\n",
        "    patience=50,\n",
        "    name='demo_run'  # T√™n l·∫ßn ch·∫°y\n",
        "    # plots = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeZv-zHkg-EZ"
      },
      "outputs": [],
      "source": [
        "# df = pd.DataFrame()\n",
        "run_dir = results.save_dir\n",
        "print(f\"Run Dir {run_dir}\")\n",
        "results_csv_path = os.path.join(run_dir, 'results.csv')\n",
        "\n",
        "print(f\"\\n3. Hu·∫•n luy·ªán ho√†n t·∫•t. ƒê∆∞·ªùng d·∫´n k·∫øt qu·∫£: {run_dir}\")\n",
        "\n",
        "# --- Tr√≠ch xu·∫•t d·ªØ li·ªáu mAP t·ª´ file CSV ---\n",
        "if os.path.exists(results_csv_path):\n",
        "    print(\"4. ƒêang ƒë·ªçc v√† tr√≠ch xu·∫•t ch·ªâ s·ªë mAP t·ª´ results.csv...\")\n",
        "\n",
        "    # ƒê·ªçc file CSV, b·ªè qua 6 d√≤ng ƒë·∫ßu ti√™n (ch·ª©a comments/header kh√¥ng c·∫ßn thi·∫øt)\n",
        "    df = pd.read_csv(results_csv_path)\n",
        "\n",
        "# Thong tin columns cua df\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaJFkEUNhrsg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def create_custom_subplot_layout():\n",
        "    \"\"\"\n",
        "    T·∫°o m·ªôt figure Matplotlib v·ªõi b·ªë c·ª•c 2 h√†ng:\n",
        "    - H√†ng 1: 1 bi·ªÉu ƒë·ªì ch√≠nh (chi·∫øm to√†n b·ªô chi·ªÅu r·ªông).\n",
        "    - H√†ng 2: 3 bi·ªÉu ƒë·ªì ph·ª•.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Kh·ªüi t·∫°o Figure\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    fig.suptitle('Detection Metrics', fontsize=16)\n",
        "\n",
        "    # 2. ƒê·ªãnh nghƒ©a GridSpec (L∆∞·ªõi 2 H√†ng x 3 C·ªôt)\n",
        "    # Ch√∫ng ta c·∫ßn 3 c·ªôt ƒë·ªÉ ch·ª©a 3 bi·ªÉu ƒë·ªì ph·ª• ·ªü h√†ng d∆∞·ªõi.\n",
        "    gs = gridspec.GridSpec(2, 3, figure=fig)\n",
        "\n",
        "    # 3. Bi·ªÉu ƒë·ªì Ch√≠nh (H√†ng 1)\n",
        "    # Chi·∫øm h√†ng 0 (ƒë·∫ßu ti√™n), v√† t·∫•t c·∫£ c√°c c·ªôt (t·ª´ 0 ƒë·∫øn 2)\n",
        "    ax_main = fig.add_subplot(gs[0, :])\n",
        "\n",
        "    # T·∫°o d·ªØ li·ªáu gi·∫£ ƒë·ªãnh\n",
        "    X = np.arange(1, 101)\n",
        "    Y1 = df['metrics/mAP50(B)']\n",
        "    Y2 = df['metrics/mAP50-95(B)']\n",
        "\n",
        "    # V·∫Ω bi·ªÉu ƒë·ªì ch√≠nh\n",
        "    ax_main.plot(X, Y1, color='darkblue', linewidth=2)\n",
        "    ax_main.plot(X, Y2, color='darkgreen', linewidth=2)\n",
        "    # ax_main.bar(X, df['time'], width= 2)\n",
        "    ax_main.set_title('mAP50 | mAP50-95')\n",
        "    ax_main.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "    ax_main.set_xlabel('Epochs')\n",
        "    ax_main.set_ylabel('Accuracy')\n",
        "\n",
        "    # 4. Ba Bi·ªÉu ƒë·ªì Ph·ª• (H√†ng 2)\n",
        "\n",
        "    # Bi·ªÉu ƒë·ªì ph·ª• 1 (H√†ng 1, C·ªôt 0)\n",
        "    ax_sub1 = fig.add_subplot(gs[1, 0])\n",
        "    ax_sub1.plot(X, df['train/box_loss'], color='skyblue')\n",
        "    ax_sub1.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "    ax_sub1.set_title('Train Box Loss')\n",
        "\n",
        "    # Bi·ªÉu ƒë·ªì ph·ª• 2 (H√†ng 1, C·ªôt 1)\n",
        "    ax_sub2 = fig.add_subplot(gs[1, 1])\n",
        "    ax_sub2.plot(X, df['train/cls_loss'], color='red')\n",
        "    ax_sub2.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "    ax_sub2.set_title('Train Class Loss')\n",
        "\n",
        "    # Bi·ªÉu ƒë·ªì ph·ª• 3 (H√†ng 1, C·ªôt 2)\n",
        "    ax_sub3 = fig.add_subplot(gs[1, 2])\n",
        "    ax_sub3.plot(X, df['train/dfl_loss'], color='green')\n",
        "    ax_sub3.set_title('Train dfl Loss')\n",
        "    ax_sub3.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "    # 5. ƒêi·ªÅu ch·ªânh kho·∫£ng c√°ch gi·ªØa c√°c bi·ªÉu ƒë·ªì ƒë·ªÉ tr√°nh ch·ªìng l·∫•n\n",
        "    fig.tight_layout(rect=[0, 0.03, 1, 0.95]) # ƒêi·ªÅu ch·ªânh cho ti√™u ƒë·ªÅ ch√≠nh\n",
        "\n",
        "    # 6. Hi·ªÉn th·ªã Figure\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY4up6kAjC1_"
      },
      "outputs": [],
      "source": [
        "    create_custom_subplot_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3oWDE-unq0h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}